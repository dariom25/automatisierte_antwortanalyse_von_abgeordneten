{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating the data\n",
    "For the annotations of the sample I use the quantative content analysis (Lamnek 2005). Here three categories will be formed:\n",
    "1. non-answer: The category encompasses every response where no reaction to the question occurs. Example: \"\"\n",
    "2. evasive answer: This category is defined as reacting to the question in not or just partly answering the question. Example: \"Sehr geehrter Herr W., haben Sie vielen Dank für Ihre Anfrage. Ich beteilige mich nicht länger am Portal abgeordnetenwatch.de. Um Ihre Frage dennoch zu beantworten, bitte ich um Mitteilung Ihrer E-Mail-Adresse an antje.tillmann@bundestag.de. Mit freundlichen Grüßen Antje Tillmann MdB\"\n",
    "3. answer: Every response which contains the answer to the questions in annotated in this category. Expample: \"Sehr geehrter Herr Schellerich,die gesamte Fraktion DIE LINKE im Deutschen Bundestag wird dem ESM-Vertrag nicht zustimmen. Ich habe dies in meiner Rede vom 29.März im Bundestag auch versucht zu begründen. Mit freundlichen Grüßen Dr. Gysi\"\n",
    "\n",
    "The drawn sample will be mannualy annotated. Next the sample will be used to categorise the rest of the answers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries for data manipulation\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "# ML: Train/test splits, cross validation,\n",
    "# gridsearch\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "# load libraries for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load libraries for text cleaning\n",
    "import spacy\n",
    "import ufal.udpipe\n",
    "from gensim.models import KeyedVectors, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import conllu\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#import eli5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_df = pd.read_csv(\"./data/stratified_sample.csv\")\n",
    "\n",
    "sample_df_2 = pd.read_csv(\"./data/stratified_sample_2.csv\", sep=\";\")\n",
    "\n",
    "sample_df = pd.concat([sample_df, sample_df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN for tokenizer to work\n",
    "sample_df = sample_df.dropna(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.drop_duplicates([\"answer\", \"question_text\", \"party\", \"first_name\", \"last_name\", \"question_teaser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the preprocessing of the data. All answers will be converted to lowercase, punctuation and other noise will be removed. Lowercasing each word has the advantage that there no two different writing styles of a word. I.e. \"die\" and \"Die\" are now recognized as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the answers to make the analysis easier\n",
    "columns_to_process = [\"answer\", \"question_text\", \"question_teaser\"]\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.lower())\n",
    "\n",
    "# remove links and punctuation\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.replace(r\"\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", regex=True))\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.replace(r\"\\s+\", \" \", regex=True))\n",
    "\n",
    "# combine the columns \"answer\", \"question_text\" and \"question_teaser\" to one column\n",
    "sample_df[\"combined\"] = sample_df[[\"question_text\", \"question_teaser\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code chunk the sample data will be split into a training and test set. On the data of the training set the model will train and with the testing set the trained model will be tested. This step is necessary to avoid overfitting and ensure the quality of the results. This classifier functions as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline\n",
    "In the next step different pipelines are created to efficiently test and tune different vectorizers and classifiers. First the CountVectorizer(), the TfidfVectorizer(), the MultinomialNB() and the LogisticRegression() are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing set with a testing set size of 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"answer\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipes_and_grids = [\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }     \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "for pipe_and_grid in pipes_and_grids:\n",
    "    search = GridSearchCV(\n",
    "        estimator=pipe_and_grid[\"pipeline\"], n_jobs=-1, param_grid=pipe_and_grid[\"grid\"], scoring=\"accuracy\", cv=10\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    pred = search.predict(X_test)\n",
    "\n",
    "    print(f\"Vectorizer and classifier: {pipe_and_grid['pipeline']}\")\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "    rep = metrics.classification_report(y_test, pred)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemming\n",
    "- svm, knearest neighbors, gradient boost\n",
    "- word embeddings\n",
    "- question_text/_teaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sehr geehrter herr r vielen dank für ihre frage!die auskunftspflicht nach § 36 absatz 3 infektionsschutzgesetz gilt nur für die in § 36 absatz 1 und absatz 2 des infektionsschutzgesetzes genannten einrichtungen und unternehmen aus dem grunde weil wir es dort mit besonders gefährdeten personen zu tun haben (bspw in kitas heimen pflegeeinrichtungen) daher ist der impfstatus der abgeordneten nicht statistisch erfasst unter ist ersichtlich dass im bundestag die 3-g-regelung umgesetzt wird hier ist auch festgehalten: \"die im rahmen von nummern 3 bis 8 erhobenen personenbezogenen daten dürfen ausschließlich zum zweck der zutrittskontrollen verarbeitet werden sie werden nicht gespeichert \"alles gute und freundliche grüße team martin dulig'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      2\u001b[0m     sample_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     sample_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      5\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#vectorizer = TfidfVectorizer(\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#    max_df=0.5,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#    min_df=5,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#text_train = vectorizer.fit(X_train)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#text_test = vectorizer.fit(X_test)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1007\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:746\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    744\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'sehr geehrter herr r vielen dank für ihre frage!die auskunftspflicht nach § 36 absatz 3 infektionsschutzgesetz gilt nur für die in § 36 absatz 1 und absatz 2 des infektionsschutzgesetzes genannten einrichtungen und unternehmen aus dem grunde weil wir es dort mit besonders gefährdeten personen zu tun haben (bspw in kitas heimen pflegeeinrichtungen) daher ist der impfstatus der abgeordneten nicht statistisch erfasst unter ist ersichtlich dass im bundestag die 3-g-regelung umgesetzt wird hier ist auch festgehalten: \"die im rahmen von nummern 3 bis 8 erhobenen personenbezogenen daten dürfen ausschließlich zum zweck der zutrittskontrollen verarbeitet werden sie werden nicht gespeichert \"alles gute und freundliche grüße team martin dulig'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"answer\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "text_train = vectorizer.fit(X_train)\n",
    "text_test = vectorizer.fit(X_test)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(text_train)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
