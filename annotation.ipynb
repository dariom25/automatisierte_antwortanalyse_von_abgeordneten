{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating the data\n",
    "For the annotations of the sample I use the quantative content analysis (Lamnek 2005). Here three categories will be formed:\n",
    "1. non-answer: The category encompasses every response where no reaction to the question occurs. Example: \"\"\n",
    "2. evasive answer: This category is defined as reacting to the question in not or just partly answering the question. Example: \"Sehr geehrter Herr W., haben Sie vielen Dank für Ihre Anfrage. Ich beteilige mich nicht länger am Portal abgeordnetenwatch.de. Um Ihre Frage dennoch zu beantworten, bitte ich um Mitteilung Ihrer E-Mail-Adresse an antje.tillmann@bundestag.de. Mit freundlichen Grüßen Antje Tillmann MdB\"\n",
    "3. answer: Every response which contains the answer to the questions in annotated in this category. Expample: \"Sehr geehrter Herr Schellerich,die gesamte Fraktion DIE LINKE im Deutschen Bundestag wird dem ESM-Vertrag nicht zustimmen. Ich habe dies in meiner Rede vom 29.März im Bundestag auch versucht zu begründen. Mit freundlichen Grüßen Dr. Gysi\"\n",
    "\n",
    "The drawn sample will be mannualy annotated. Next the sample will be used to categorise the rest of the answers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries for data manipulation\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "# load libraries for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# load libraries for text cleaning\n",
    "import spacy\n",
    "import ufal.udpipe\n",
    "from gensim.models import KeyedVectors, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import conllu\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#import eli5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_df = pd.read_csv(\"./data/stratified_sample.csv\")\n",
    "# remove NaN for tokenizer to work\n",
    "sample_df = sample_df.dropna(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the preprocessing of the data. All answers will be converted to lowercase, stopwords will be removed, as well as punctuation and other noise. This step necessary since the most commons words are \"die\", \"mit\" and so on. These words have no inherent meaning and are not useful for further analysis. Lowercasing each word has the advantage that there no two different writing styles of a word. I.e. \"die\" and \"Die\" are now recognized as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the answers to make the analysis easier\n",
    "sample_df[\"answer\"] = sample_df[\"answer\"].str.lower()\n",
    "\n",
    "# remove links and punctuation\n",
    "sample_df[\"answer\"] = sample_df[\"answer\"].str.replace(r\"\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", regex=True)\n",
    "sample_df[\"answer\"] = sample_df[\"answer\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "sample_df.at[10, \"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
