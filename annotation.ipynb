{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating the data\n",
    "For the annotations of the sample I use the quantative content analysis (Lamnek 2005). Here three categories will be formed:\n",
    "1. non-answer: The category encompasses every response where no reaction to the question occurs. Example: \"\"\n",
    "2. evasive answer: This category is defined as reacting to the question in not or just partly answering the question. Example: \"Sehr geehrter Herr W., haben Sie vielen Dank für Ihre Anfrage. Ich beteilige mich nicht länger am Portal abgeordnetenwatch.de. Um Ihre Frage dennoch zu beantworten, bitte ich um Mitteilung Ihrer E-Mail-Adresse an antje.tillmann@bundestag.de. Mit freundlichen Grüßen Antje Tillmann MdB\"\n",
    "3. answer: Every response which contains the answer to the questions in annotated in this category. Expample: \"Sehr geehrter Herr Schellerich,die gesamte Fraktion DIE LINKE im Deutschen Bundestag wird dem ESM-Vertrag nicht zustimmen. Ich habe dies in meiner Rede vom 29.März im Bundestag auch versucht zu begründen. Mit freundlichen Grüßen Dr. Gysi\"\n",
    "\n",
    "The drawn sample will be mannualy annotated. Next the sample will be used to categorise the rest of the answers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries for data manipulation\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "# ML: Train/test splits, cross validation,\n",
    "# gridsearch\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "# load libraries for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# load libraries for text cleaning\n",
    "import spacy\n",
    "import ufal.udpipe\n",
    "from gensim.models import KeyedVectors, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import conllu\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#import eli5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_df = pd.read_csv(\"./data/stratified_sample.csv\")\n",
    "\n",
    "sample_df_2 = pd.read_csv(\"./data/stratified_sample_2.csv\", sep=\";\")\n",
    "\n",
    "sample_df = pd.concat([sample_df, sample_df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN for tokenizer to work\n",
    "sample_df = sample_df.dropna(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.drop_duplicates([\"answer\", \"question_text\", \"party\", \"first_name\", \"last_name\", \"question_teaser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the preprocessing of the data. All answers will be converted to lowercase, punctuation and other noise will be removed. Lowercasing each word has the advantage that there no two different writing styles of a word. I.e. \"die\" and \"Die\" are now recognized as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the answers to make the analysis easier\n",
    "columns_to_process = [\"answer\", \"question_text\", \"question_teaser\"]\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.lower())\n",
    "\n",
    "# remove links and punctuation\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.replace(r\"\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", regex=True))\n",
    "sample_df[columns_to_process] = sample_df[columns_to_process].apply(lambda col: col.str.replace(r\"\\s+\", \" \", regex=True))\n",
    "\n",
    "# combine the columns \"answer\", \"question_text\" and \"question_teaser\" to one column\n",
    "sample_df[\"combined\"] = sample_df[[\"question_text\", \"question_teaser\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code chunk the sample data will be split into a training and test set. On the data of the training set the model will train and with the testing set the trained model will be tested. This step is necessary to avoid overfitting and ensure the quality of the results. This classifier functions as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.71      0.95      0.81       242\n",
      "evasive answer       0.75      0.30      0.43       131\n",
      "\n",
      "      accuracy                           0.72       373\n",
      "     macro avg       0.73      0.62      0.62       373\n",
      "  weighted avg       0.73      0.72      0.68       373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing set with a testing set size of 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"answer\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# vectorizer with stopwords\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stopwords.words(\"german\")\n",
    ")\n",
    "\n",
    "text_train = vectorizer.fit_transform(X_train)\n",
    "text_test = vectorizer.transform(X_test)\n",
    "\n",
    "# create mulitnominal naive bayes classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(text_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(text_test)\n",
    "\n",
    "rep = metrics.classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline\n",
    "In the next step a pipeline is created to efficiently test and tune different vectorizers and classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__min_df': 10, 'vectorizer__ngram_range': (1, 1), 'vectorizer__stop_words': None}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.71      0.95      0.81       242\n",
      "evasive answer       0.75      0.30      0.43       131\n",
      "\n",
      "      accuracy                           0.72       373\n",
      "     macro avg       0.73      0.62      0.62       373\n",
      "  weighted avg       0.73      0.72      0.68       373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing set with a testing set size of 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"answer\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# create pipeline with vectorizer and classifier\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", CountVectorizer()),\n",
    "        (\"classifier\", MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create grid with different preprocessing steps\n",
    "grid = {\n",
    "    \"vectorizer__stop_words\" : [None, stopwords.words(\"german\")],\n",
    "    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "    \"vectorizer__min_df\" : [1, 5],\n",
    "    #\"classifier__C\" : [0.01, 1, 100]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=\"accuracy\", cv=10\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "rep = metrics.classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
