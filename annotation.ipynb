{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating the data\n",
    "For the annotations of the sample I use the quantative content analysis (Lamnek 2005). Here three categories will be formed:\n",
    "1. non-answer: The category encompasses every response where no reaction to the question occurs. Example: \"\"\n",
    "2. evasive answer: This category is defined as reacting to the question in not or just partly answering the question. Example: \"Sehr geehrter Herr W., haben Sie vielen Dank für Ihre Anfrage. Ich beteilige mich nicht länger am Portal abgeordnetenwatch.de. Um Ihre Frage dennoch zu beantworten, bitte ich um Mitteilung Ihrer E-Mail-Adresse an antje.tillmann@bundestag.de. Mit freundlichen Grüßen Antje Tillmann MdB\"\n",
    "3. answer: Every response which contains the answer to the questions in annotated in this category. Expample: \"Sehr geehrter Herr Schellerich,die gesamte Fraktion DIE LINKE im Deutschen Bundestag wird dem ESM-Vertrag nicht zustimmen. Ich habe dies in meiner Rede vom 29.März im Bundestag auch versucht zu begründen. Mit freundlichen Grüßen Dr. Gysi\"\n",
    "\n",
    "The drawn sample will be mannualy annotated. Next the sample will be used to categorise the rest of the answers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries for data manipulation\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "# ML: Train/test splits, cross validation,\n",
    "# gridsearch\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "# load libraries for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load libraries for text cleaning\n",
    "import spacy\n",
    "from spacy.lang.de.examples import sentences\n",
    "# python -m spacy download de_core_news_sm\n",
    "import ufal.udpipe\n",
    "from gensim.models import KeyedVectors, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import conllu\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_df = pd.read_csv(\"./data/stratified_sample.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN for tokenizer to work\n",
    "sample_df = sample_df.dropna(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisierung von Umlauten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_umlauts(text):    \n",
    "    umlauts = {\n",
    "        \"ae\" : \"ä\",\n",
    "        \"oe\" : \"ö\",\n",
    "        \"ue\" : \"ü\",\n",
    "        \"ss\" : \"ß\"\n",
    "    }\n",
    "\n",
    "    for repl, original in umlauts.items():\n",
    "        text = text.replace(original, repl)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"clean_answers\"] = sample_df[\"answer\"].apply(remove_umlauts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the preprocessing of the data. All answers will be converted to lowercase, punctuation and other noise will be removed. Lowercasing each word has the advantage that there no two different writing styles of a word. I.e. \"die\" and \"Die\" are now recognized as the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def text_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # remove links, punctuation, special letters\n",
    "    text = re.sub(r\"[^a-zA-Z]|\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", text)\n",
    "    \n",
    "    # replace single characters\n",
    "    text = re.sub(r\" [a-zA-Z] \", \" \", text)\n",
    "    \n",
    "    # remove additional whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # lemmatize texts\n",
    "    text = text_lemmatization(text)\n",
    "\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization of words\n",
    "    text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    german_stopwords = set(stopwords.words(\"german\"))\n",
    "    text = [w for w in text if w not in german_stopwords]\n",
    "    \n",
    "    # return joined text\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"clean_answers\"] = sample_df[\"clean_answers\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       geehrt herr vieler dank nachricht versorgung s...\n",
       "1       geehrt herr vieler dank frage frage wertvoll f...\n",
       "2       geehrt herr anpassung besoldung regelung infla...\n",
       "3       geehrt frau vieler dank nachricht frage kranke...\n",
       "4       geehrt herr blick moria schmerzen tatsache obd...\n",
       "                              ...                        \n",
       "2349    geehrt herr herzlich dank fuer frage februar b...\n",
       "2350    geehrt herr vieler dank fuer frage bitte verst...\n",
       "2352    geehrt herr burger bitte verstaendni herr lind...\n",
       "2353    afd stehen fuer verkleinerung parlament vorsch...\n",
       "2354    geehrt herr roth offensichtlich manipulation z...\n",
       "Name: clean_answers, Length: 1867, dtype: object"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"clean_answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords with refined list and after lemmatization to catch \n",
    "def remove_individual_stopwords(text):\n",
    "    individual_stopwords = [\"fuer\", \"gr\", \"nnen\", \"soll\", \"mehr\", \"weit\", \"freundlich\", \"herr\", \"frage\", \"frau\", \"dank\", \"ber\", \"gut\", \"vieler\", \"werden\", \"jahr\", \"viele\", \"ehre\", \"geehrt\", \"herzlich\", \"frage\", \"jahr\", \"geben\"]\n",
    "    text = text.split()\n",
    "    text = [w for w in text if w not in individual_stopwords]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "sample_df[\"clean_answers\"] = sample_df[\"clean_answers\"].apply(remove_individual_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       nachricht versorgung sauber trinkwasser wesent...\n",
       "1       wertvoll fachlich zust ndigkeit parlamentarisc...\n",
       "2       anpassung besoldung regelung inflationspr mie ...\n",
       "3       nachricht krankenversicherungsbeitr gen fallen...\n",
       "4       blick moria schmerzen tatsache obdachlos gewor...\n",
       "                              ...                        \n",
       "2349    februar bundeskanzler olaf scholz praesident s...\n",
       "2350    bitte verstaendni dafu beruflich entscheidung ...\n",
       "2352    burger bitte verstaendni lindner kapazitaet za...\n",
       "2353    afd stehen verkleinerung parlament vorschlag v...\n",
       "2354    roth offensichtlich manipulation zustande geko...\n",
       "Name: clean_answers, Length: 1867, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"clean_answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequencies</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>34.532199</td>\n",
       "      <td>sollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>31.233002</td>\n",
       "      <td>mensch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>30.417955</td>\n",
       "      <td>deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>29.573698</td>\n",
       "      <td>bundestag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>27.049973</td>\n",
       "      <td>afd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>26.982804</td>\n",
       "      <td>deutsch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>26.100131</td>\n",
       "      <td>spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>24.961109</td>\n",
       "      <td>bundesregierung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>24.418855</td>\n",
       "      <td>glich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>23.814459</td>\n",
       "      <td>finden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      frequencies            index\n",
       "3690    34.532199           sollen\n",
       "2767    31.233002           mensch\n",
       "1034    30.417955      deutschland\n",
       "874     29.573698        bundestag\n",
       "79      27.049973              afd\n",
       "1032    26.982804          deutsch\n",
       "3726    26.100131              spd\n",
       "872     24.961109  bundesregierung\n",
       "1940    24.418855            glich\n",
       "1641    23.814459           finden"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show most frequent words\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=5,\n",
    "    max_df=0.5\n",
    ")\n",
    "dtm = vectorizer.fit_transform(sample_df[\"clean_answers\"])\n",
    "\n",
    "frequencies = dtm.sum(axis=0).tolist()[0]\n",
    "\n",
    "freq_df = pd.DataFrame(\n",
    "    dict(frequencies=frequencies,\n",
    "         index=vectorizer.get_feature_names_out()\n",
    "    )\n",
    ")\n",
    "\n",
    "freq_df.sort_values(\"frequencies\", ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code chunk the sample data will be split into a training and test set. On the data of the training set the model will train and with the testing set the trained model will be tested. This step is necessary to avoid overfitting and ensure the quality of the results. This classifier functions as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline\n",
    "In the next step different pipelines are created to efficiently test and tune different vectorizers and classifiers. First the CountVectorizer(), the TfidfVectorizer(), the MultinomialNB() and the LogisticRegression() are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', MultinomialNB())])\n",
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.75      0.94      0.84       253\n",
      "evasive answer       0.75      0.35      0.47       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.75      0.65      0.66       374\n",
      "  weighted avg       0.75      0.75      0.72       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', MultinomialNB())])\n",
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.73      0.98      0.83       253\n",
      "evasive answer       0.83      0.24      0.37       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.78      0.61      0.60       374\n",
      "  weighted avg       0.76      0.74      0.68       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Best parameters: {'classifier__C': 0.01, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 3)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.74      0.94      0.83       253\n",
      "evasive answer       0.72      0.32      0.45       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.73      0.63      0.64       374\n",
      "  weighted avg       0.74      0.74      0.71       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Best parameters: {'classifier__C': 1, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.74      0.97      0.84       253\n",
      "evasive answer       0.81      0.29      0.43       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.78      0.63      0.63       374\n",
      "  weighted avg       0.76      0.75      0.71       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', RandomForestClassifier())])\n",
      "Best parameters: {'vectorizer__max_df': 1.0, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.75      0.96      0.84       253\n",
      "evasive answer       0.80      0.32      0.46       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.77      0.64      0.65       374\n",
      "  weighted avg       0.76      0.75      0.72       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', RandomForestClassifier())])\n",
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__max_features': 5000, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.75      0.91      0.82       253\n",
      "evasive answer       0.67      0.36      0.47       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.71      0.64      0.65       374\n",
      "  weighted avg       0.72      0.74      0.71       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing set with a testing set size of 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"clean_answers\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipes_and_grids = [\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }     \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"vectorizer__max_features\" : [1000, 5000, 10000]\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "for pipe_and_grid in pipes_and_grids:\n",
    "    search = GridSearchCV(\n",
    "        estimator=pipe_and_grid[\"pipeline\"], n_jobs=-1, param_grid=pipe_and_grid[\"grid\"], scoring=\"accuracy\", cv=10\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    pred = search.predict(X_test)\n",
    "\n",
    "    print(f\"Vectorizer and classifier: {pipe_and_grid['pipeline']}\")\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "    rep = metrics.classification_report(y_test, pred)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svm, knearest neighbors, gradient boost\n",
    "- word embeddings\n",
    "- question_text/_teaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.76      0.94      0.84       253\n",
      "evasive answer       0.76      0.37      0.50       121\n",
      "\n",
      "      accuracy                           0.76       374\n",
      "     macro avg       0.76      0.66      0.67       374\n",
      "  weighted avg       0.76      0.76      0.73       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"clean_answers\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train_vect, y_train)\n",
    "y_pred = svm.predict(X_test_vect)\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
