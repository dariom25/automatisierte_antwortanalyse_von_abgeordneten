{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating the data\n",
    "For the annotations of the sample I use the quantative content analysis (Lamnek 2005). Here three categories will be formed:\n",
    "1. non-answer: The category encompasses every response where no reaction to the question occurs. Example: \"\"\n",
    "2. evasive answer: This category is defined as reacting to the question in not or just partly answering the question. Example: \"Sehr geehrter Herr W., haben Sie vielen Dank für Ihre Anfrage. Ich beteilige mich nicht länger am Portal abgeordnetenwatch.de. Um Ihre Frage dennoch zu beantworten, bitte ich um Mitteilung Ihrer E-Mail-Adresse an antje.tillmann@bundestag.de. Mit freundlichen Grüßen Antje Tillmann MdB\"\n",
    "3. answer: Every response which contains the answer to the questions in annotated in this category. Expample: \"Sehr geehrter Herr Schellerich,die gesamte Fraktion DIE LINKE im Deutschen Bundestag wird dem ESM-Vertrag nicht zustimmen. Ich habe dies in meiner Rede vom 29.März im Bundestag auch versucht zu begründen. Mit freundlichen Grüßen Dr. Gysi\"\n",
    "\n",
    "The drawn sample will be mannualy annotated. Next the sample will be used to categorise the rest of the answers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries for data manipulation\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "# ML: Train/test splits, cross validation,\n",
    "# gridsearch\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "# load libraries for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load libraries for text cleaning\n",
    "import spacy\n",
    "from spacy.lang.de.examples import sentences\n",
    "# python -m spacy download de_core_news_sm\n",
    "import ufal.udpipe\n",
    "from gensim.models import KeyedVectors, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "import conllu\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "#import eli5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample_df = pd.read_csv(\"./data/stratified_sample.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN for tokenizer to work\n",
    "sample_df = sample_df.dropna(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.drop_duplicates([\"answer\", \"question_text\", \"party\", \"first_name\", \"last_name\", \"question_teaser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisierung von Umlauten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_umlauts(text):    \n",
    "    umlauts = {\n",
    "        \"ae\" : \"ä\",\n",
    "        \"oe\" : \"ö\",\n",
    "        \"ue\" : \"ü\",\n",
    "        \"ss\" : \"ß\"\n",
    "    }\n",
    "\n",
    "    for repl, original in umlauts.items():\n",
    "        text = text.replace(original, repl)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"clean_answers\"] = sample_df[\"answer\"].apply(remove_umlauts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the preprocessing of the data. All answers will be converted to lowercase, punctuation and other noise will be removed. Lowercasing each word has the advantage that there no two different writing styles of a word. I.e. \"die\" and \"Die\" are now recognized as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # remove links, punctuation, special letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]|\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", text)\n",
    "    \n",
    "    # remove additional whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization of words\n",
    "    words = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    german_stopwords = set(stopwords.words(\"german\"))\n",
    "    words = [w for w in words if w not in german_stopwords]\n",
    "    \n",
    "    # return joined text\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"clean_answers\"] = sample_df[\"clean_answers\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def text_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"clean_answers\"] = sample_df[\"clean_answers\"].apply(text_lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code chunk the sample data will be split into a training and test set. On the data of the training set the model will train and with the testing set the trained model will be tested. This step is necessary to avoid overfitting and ensure the quality of the results. This classifier functions as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline\n",
    "In the next step different pipelines are created to efficiently test and tune different vectorizers and classifiers. First the CountVectorizer(), the TfidfVectorizer(), the MultinomialNB() and the LogisticRegression() are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', MultinomialNB())])\n",
      "Best parameters: {'vectorizer__max_df': 1.0, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.75      0.94      0.84       253\n",
      "evasive answer       0.74      0.36      0.48       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.75      0.65      0.66       374\n",
      "  weighted avg       0.75      0.75      0.72       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', MultinomialNB())])\n",
      "Best parameters: {'vectorizer__max_df': 1.0, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.73      0.98      0.83       253\n",
      "evasive answer       0.83      0.24      0.37       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.78      0.61      0.60       374\n",
      "  weighted avg       0.76      0.74      0.68       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Best parameters: {'classifier__C': 0.01, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 3)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.74      0.93      0.83       253\n",
      "evasive answer       0.70      0.32      0.44       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.72      0.63      0.63       374\n",
      "  weighted avg       0.73      0.74      0.70       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', LogisticRegression())])\n",
      "Best parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 2)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.76      0.91      0.83       253\n",
      "evasive answer       0.67      0.39      0.49       121\n",
      "\n",
      "      accuracy                           0.74       374\n",
      "     macro avg       0.71      0.65      0.66       374\n",
      "  weighted avg       0.73      0.74      0.72       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
      "                ('classifier', RandomForestClassifier())])\n",
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.74      0.96      0.84       253\n",
      "evasive answer       0.80      0.30      0.43       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.77      0.63      0.64       374\n",
      "  weighted avg       0.76      0.75      0.71       374\n",
      "\n",
      "Vectorizer and classifier: Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
      "                ('classifier', RandomForestClassifier())])\n",
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__max_features': 10000, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        answer       0.76      0.92      0.83       253\n",
      "evasive answer       0.71      0.40      0.51       121\n",
      "\n",
      "      accuracy                           0.75       374\n",
      "     macro avg       0.73      0.66      0.67       374\n",
      "  weighted avg       0.74      0.75      0.73       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing set with a testing set size of 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"clean_answers\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipes_and_grids = [\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }     \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", MultinomialNB())\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"classifier__C\" : [0.01, 1, 100]\n",
    "        }  \n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", CountVectorizer()),\n",
    "                (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"pipeline\" : Pipeline(\n",
    "            steps=[\n",
    "                (\"vectorizer\", TfidfVectorizer()),\n",
    "                (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "            ]\n",
    "        ),\n",
    "        \"grid\" : {\n",
    "                    \"vectorizer__ngram_range\" : [(1,1), (1,2), (1,3)],\n",
    "                    \"vectorizer__max_df\" : [0.5, 1.0],\n",
    "                    \"vectorizer__min_df\" : [1, 5],\n",
    "                    \"vectorizer__max_features\" : [1000, 5000, 10000]\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "for pipe_and_grid in pipes_and_grids:\n",
    "    search = GridSearchCV(\n",
    "        estimator=pipe_and_grid[\"pipeline\"], n_jobs=-1, param_grid=pipe_and_grid[\"grid\"], scoring=\"accuracy\", cv=10\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    pred = search.predict(X_test)\n",
    "\n",
    "    print(f\"Vectorizer and classifier: {pipe_and_grid['pipeline']}\")\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "    rep = metrics.classification_report(y_test, pred)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svm, knearest neighbors, gradient boost\n",
    "- word embeddings\n",
    "- question_text/_teaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'TfidfVectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m text_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[0;32m     15\u001b[0m text_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit(X_test)\n\u001b[1;32m---> 17\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1007\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:746\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    744\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'TfidfVectorizer'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[\"answer\"],\n",
    "    sample_df[\"answer_encoded\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "text_train = vectorizer.fit(X_train)\n",
    "text_test = vectorizer.fit(X_test)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(text_train)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
