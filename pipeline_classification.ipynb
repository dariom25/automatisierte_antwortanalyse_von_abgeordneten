{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fasttext.util\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline für Textklassifikation mit fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "sample = pd.read_csv(\"data/labeled_unprocessed_sample_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove possible missing values and duplicates\n",
    "sample = sample.dropna()\n",
    "sample = sample.drop_duplicates(subset=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def text_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # remove links, punctuation, special letters\n",
    "    text = re.sub(r\"[^a-zA-öZÖäÄüÜß]|\\bhttps?://\\S*|&\\w+;|[\\.,]\", \" \", text)\n",
    "    \n",
    "    # replace single characters\n",
    "    text = re.sub(r\" [a-zA-Z] \", \" \", text)\n",
    "    \n",
    "    # remove additional whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # lemmatize texts\n",
    "    text = text_lemmatization(text)\n",
    "\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization of words\n",
    "    text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    german_stopwords = set(stopwords.words(\"german\"))\n",
    "    text = [w for w in text if w not in german_stopwords]\n",
    "    \n",
    "    # return joined text\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"clean_answer\"] = sample[\"answer\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequencies</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>2012</td>\n",
       "      <td>geehrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8766</th>\n",
       "      <td>1718</td>\n",
       "      <td>freundlich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>1712</td>\n",
       "      <td>vieler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11383</th>\n",
       "      <td>1698</td>\n",
       "      <td>herr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594</th>\n",
       "      <td>1579</td>\n",
       "      <td>frage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>1279</td>\n",
       "      <td>dank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20807</th>\n",
       "      <td>971</td>\n",
       "      <td>sollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15637</th>\n",
       "      <td>942</td>\n",
       "      <td>mensch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9138</th>\n",
       "      <td>900</td>\n",
       "      <td>geben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10810</th>\n",
       "      <td>898</td>\n",
       "      <td>gut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12650</th>\n",
       "      <td>894</td>\n",
       "      <td>jahr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>842</td>\n",
       "      <td>müssen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>814</td>\n",
       "      <td>deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>794</td>\n",
       "      <td>mehr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>755</td>\n",
       "      <td>frau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25599</th>\n",
       "      <td>664</td>\n",
       "      <td>weit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>654</td>\n",
       "      <td>bundestag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5284</th>\n",
       "      <td>646</td>\n",
       "      <td>deutsch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>582</td>\n",
       "      <td>dafür</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>577</td>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequencies        index\n",
       "9221          2012       geehrt\n",
       "8766          1718   freundlich\n",
       "24838         1712       vieler\n",
       "11383         1698         herr\n",
       "8594          1579        frage\n",
       "4977          1279         dank\n",
       "20807          971       sollen\n",
       "15637          942       mensch\n",
       "9138           900        geben\n",
       "10810          898          gut\n",
       "12650          894         jahr\n",
       "16363          842       müssen\n",
       "5287           814  deutschland\n",
       "15536          794         mehr\n",
       "8645           755         frau\n",
       "25599          664         weit\n",
       "4465           654    bundestag\n",
       "5284           646      deutsch\n",
       "4943           582        dafür\n",
       "14313          577         land"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show most frequent words\n",
    "vectorizer = CountVectorizer(\n",
    ")\n",
    "dtm = vectorizer.fit_transform(sample[\"clean_answer\"])\n",
    "\n",
    "frequencies = dtm.sum(axis=0).tolist()[0]\n",
    "\n",
    "df_freq = pd.DataFrame(\n",
    "    dict(frequencies=frequencies,\n",
    "         index=vectorizer.get_feature_names_out()\n",
    "    )\n",
    ")\n",
    "\n",
    "df_freq.sort_values(\"frequencies\", ascending=False).head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords with refined list and after lemmatization to catch \n",
    "def remove_individual_stopwords(text):\n",
    "    individual_stopwords = [\n",
    "        \"geehrt\",\n",
    "        \"frau\",\n",
    "        \"vieler\",\n",
    "        \"dank\",\n",
    "        \"herr\",\n",
    "        \"danke\",\n",
    "        \"anfrage\",\n",
    "        \"frage\",\n",
    "        \"nachricht\",\n",
    "        \"freundlich\",\n",
    "        \"sollen\",\n",
    "        \"müssen\",\n",
    "        \"mehr\",\n",
    "        \"grüße\",\n",
    "        \"daher\",\n",
    "        \"immer\",\n",
    "        \"dafür\",\n",
    "        \"frage\"\n",
    "    ]\n",
    "    text = text.split()\n",
    "    text = [w for w in text if w not in individual_stopwords]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"clean_answer\"] = sample[\"clean_answer\"].apply(remove_individual_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for processing by fasttext\n",
    "sample[\"answer_encoded\"] = sample[\"answer_encoded\"].apply(lambda x: x.replace(\" \", \"_\"))\n",
    "sample[\"answer_encoded\"] = sample[\"answer_encoded\"].apply(lambda x: \"__label__\" + x)\n",
    "sample[\"answer_encoding_combined\"] = sample[\"answer_encoded\"]+ \" \" + sample[\"clean_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate sample data into training, test and validation sets (80/10/10)\n",
    "training_sample, temp_df = train_test_split(sample, test_size=0.2, random_state=42)\n",
    "testing_sample, validation_sample = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export datasets\n",
    "training_sample[\"answer_encoding_combined\"].to_csv(\"data/training_data.csv\", index=False, header=False, sep=\";\")\n",
    "testing_sample[\"answer_encoding_combined\"].to_csv(\"data/testing_data.csv\", index=False, header=False, sep=\";\")\n",
    "validation_sample[\"answer_encoding_combined\"].to_csv(\"data/validation_data.csv\", index=False, header=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne optimierte Hyperparamater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model without optimizing the hyperparameter\n",
    "ft_model = fasttext.train_supervised(input=\"data/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 0.6578947368421053, 0.6578947368421053)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "ft_model.test(\"data/testing_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mit optimierten Hyperparametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with optimizing the hyperparameter\n",
    "ft_model_optimized = fasttext.train_supervised(\n",
    "    input=\"data/training_data.csv\", \n",
    "    autotuneValidationFile=\"data/validation_data.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 0.6798245614035088, 0.6798245614035088)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "ft_model_optimized.test(\"data/testing_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testergebnisse für Label \"answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6818181818181818,\n",
       " 'recall': 0.9310344827586207,\n",
       " 'f1score': 0.7871720116618076}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model for label \"answer\"\n",
    "ft_model_optimized.test_label(\"data/testing_data.csv\")[\"__label__answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testergebnisse für Label \"evasive answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6666666666666666,\n",
       " 'recall': 0.24096385542168675,\n",
       " 'f1score': 0.35398230088495575}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model for label \"evasive answer\"\n",
    "ft_model_optimized.test_label(\"data/testing_data.csv\")[\"__label__evasive_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "ft_model_optimized.save_model(\"model/classification.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
